---
title: "Machine Learning"
author: "Pradeep Kumar"
date: "30 August 2018"
output: html_document
---

# Practical Machine Learning Project

## Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise.

## Getting and Cleaning the Data

### Loading the libraries
```{r}
library(caret)
library(rattle)
library(randomForest)
```

### Reading Data
Loading the data using URL's given.
```{r}
trainingdata <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), header = T, na.strings = c("NA", ""))
testingdata <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"), header = T, na.strings = c("NA", ""))
```

### Processing Data
```{r}
str(trainingdata)
str(testingdata)
```

### Removing all columns having missing values
```{r}
trainingdata <- trainingdata[,colSums(is.na(trainingdata))==0]
testingdata <- testingdata[,colSums(is.na(testingdata))==0]
```

### Removing the data that does not have much influence
```{r}
nzv1 <- nearZeroVar(trainingdata, saveMetrics = TRUE)
nzv1
nzv2 <- nearZeroVar(testingdata, saveMetrics = TRUE)
nzv2
```

First 5 columns are just for information and does not have much influene and by the above model removing 6th and 7th column
New data

```{r}
trainingdata <- trainingdata[,-c(1:7)]
testingdata <- testingdata[,-c(1:7)]
```

```{r}
dim(trainingdata)
dim(testingdata)
```

### Fitting the training and test data 
Splitting the training data into new training and testing data of 70%-30% ratio.
```{r}
inTrain <- createDataPartition(y = trainingdata$classe, p = 0.7, list = F)
training <- trainingdata[inTrain,]
testing <- trainingdata[-inTrain,]
dim(training)
dim(testing)
```

## Fitting ML alogrithms

### Fitting a Decision Tree Model
```{r}
fit_dt <- train(classe ~ ., data = training, method = "rpart")
fancyRpartPlot(fit_dt$finalModel)
predict_dt <- predict(fit_dt, newdata = testing)
confusionMatrix(predict_dt, testing$classe)$overall[1]
```
We can see that it has an accuracy of 0.49, which is quite low.

### Fitting Random Forest Model
```{r}
fit_rf <- randomForest(classe ~ ., data = training)
predict_rf <- predict(fit_rf, newdata = testing)
confusionMatrix(predict_rf, testing$classe)$overall[1]
```
This model has an accuracy of about 0.995 , which is quiet good.

Based on all above two models we can see that random forest model has the highest accuracy.
So, using the random forest model to predict the testing data

## Prediction
```{r}
final_pred <- predict(fit_rf, newdata = testingdata)
final_pred
```

